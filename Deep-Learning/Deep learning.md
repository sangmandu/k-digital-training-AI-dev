# 심층학습(Deep learning)

<details>
<summary><b>심층학습 기초I</b></summary>   
<div markdown="1">  
   
+ **다층 퍼셉트론**에 **은닉층을 여러개 추가**하면 깊은 신경망(deep neural networks)
   > 전통적인 다층 퍼셉트론 : 얕은 구조이므로 가공하지 않은 획득한 원래 패턴을 그대로 입력하면 **낮은 성능**   
   
   > 따라서 사람이 **수작업 특징**을 선택하거나 추출해서 신경망에 입력함 
   
+ 깊은 신경망의 학습
+ 새로운 응용을 창출하고 인공지능 제품의 성능을 획기적으로 향상
   > **학습**에 의해 **자동적**으로 **데이터로부터 특징(data-driven features) 추출** : **표현학습 representation learning** ⭐   
   
   > 현대 기계학습을 주도 ✨
   
   
+ 배경
   + 퍼셉트론의 한계 → 다층 퍼셉트론 (1980년대)
      + 1980년대에 이미 깊은 신경망 아이디어가 등장했으나, **실현 불가능** 💥
         + **경사 소멸 문제(gradient vanishing problem)**
            > 층이 깊어지면서 기울기가 중간에 0이 되어서 gradient 값이 소실되는 문제 발생   
            
         + 작은 훈련집합
         + 과다한 연산과 시간소요(낮은 연산의 범용 컴퓨터, 값비싼 슈퍼컴퓨터)
      
      + 일부 연구자들은 지속적 연구 진행
         + 학습률, 은닉 노드 수에 따른 성능 변화 양상, 모멘텀과 같은 최적 탐색 방법 모색, 데이터 전처리 영향, 활성함수의 영향, 규제 기법의 영향 등   
         
   + **성공 배경**
      + 혁신적 알고리즘 등장 ✨
         + **합성곱 신경망(convolutional neural networks, CNN)** 구조
            > 부분 연결과 가중치 공유를 통해 효율적인 신경망 학습 구조 제공 
            
            > 예) MNIST 인식 경쟁이나 ILSVRC 사진 인식 경쟁에서 **CNN이 DMLP보다 확연히 우월**
            
         + 경사 소멸 문제 해결을 위한 **ReLU 활성함수**
         + 과잉적합 방지하는데 효과적인 **다양한 규제기법**
         + 층별 예비학습(pretraining) 기법 개발
      + 값싼 **GPGPU** 등장
      + 학습 데이터 **양과 질의 향상**
      
+ 깊은 신경망의 표현학습 (또는 특징학습)      
   + **낮은 단계 은닉층** : 선이나 모서리와 같은 **간단한 (저급) 특징** 추출
   + **높은 단계 은닉층** : 추상적인 형태(abstractive representation)의 **복잡한 (고급) 특징** 추출
   
   + **표현학습이 강력해짐**에 따라 기존 응용에서 **획기적인 성능 향상 ↑**  
   
      + 영상 인식, 음성 인식, 언어 번역 등   
      
      + 새로운 응용 창출 ✨
         + 분류, 회귀 뿐만 아니라 **생성 모델 / pixel 수준의 영상 분할**
         + CNN과 LSTM의 **혼합 학습 모델**(예시: 자연 영상에 주석달기 응용) 등이 가능해짐


+ 깊은 다층 퍼셉트론 (깊은 신경망, DMLP)
   > **MLP의 동작을 나타내는 식**을 보다 많은 단계로 **확장**한 것

   + 기존 MLP 학습과 유사
      + 경사도 계산과 가중치 갱신을 **더 많은 단계**에 걸쳐 수행
   
   
+ 다층 퍼셉트론의 역사적 발전 양상    


> <img src="https://user-images.githubusercontent.com/72974863/105632344-ec264080-5e95-11eb-8595-d3c49f9fcd57.png">    


### 왜 심층학습이 강력한가? 🤔
   + **종단간(end-to-end) 최적화**된 학습 가능 ❗   
      > 고전적 방법에서는 여러 단계를 따로 설계 구현해야함(분할, 특징 추출, 분류를 따로 구현)
      
      > 심층학습은 전체 깊은 신경망을 **동시에 최적화** → **종단간 학습**   
      
   + **깊이**의 중요성
      > 은닉층의 개수가 증가함에 따라 표현력 증가 ❗   
      
   + **계층적 특징**(hierarchical features) ❗ 
      + 낮은 층에서는 공통적인 간단한 특징, 다음 층은 이전 층의 조합을 통해 점진적으로 추상화하는 형태   
      
</div>
</details>
    
## 컨볼루션 (합성곱) 신경망 (convolutional neural network, CNN)
+ **영상 인식**에 많이 쓰임   
   > 영상 분야에서 다양하게 활용 (분류 classification, 검출 detection, 검색 retrieval, 분할 segmentation) ✨

+ **컴퓨터 비전**의 어려운 점
   + **동일한 객체라도** 영상을 찍는 **카메라의 이동과 각도**에 따라 **모든 픽셀값이 변화**됨
   + **경계색(보호색)** 으로 **배경과 구분이 어려운 경우**가 존재
   + **조명** 에 따른 변화
   + **기형적인 형태의 영상** 이나 **일부가 가려진 영상** 존재
   + **같은 종류 간의 변화**가 큼    
   
+ 컨볼루션 신경망
   + **컨볼루션층(CONV)**
      + 선형함수인 컨볼루션과 비선형 함수인 활성함수의 조합   
      
   + **풀링층(POOL)**
      + 컨볼루션의 얻어진 특징을 통계적으로 압축   
      
   + 전체 구조
      > <img src="https://user-images.githubusercontent.com/72974863/105667011-e70cd400-5f1d-11eb-9b4c-a80b33f4705b.png">   
      
      > [이미지 출처](https://en.wikipedia.org/wiki/Convolutional_neural_network)
   
   + DMLP vs CNN
      + DMLP : 완전 연결 구조, 높은 복잡도, 학습 매우 느림, 과잉적합 우려
      + CNN : 컨볼루션 연산을 이용한 **부분연결(희소 연결)** 구조, 복잡도 크게 낮춤, 컨볼루션 연산은 **좋은 특징 추출**   
      
      + CNN 특징 ✨
         + **격자(grid) 구조를 갖는 데이터**에 적합
            > 영상, 음성 등
         + 수용장(receptive field)은 인간시각과 유사
         + 가변 크기의 입력처리 가능
